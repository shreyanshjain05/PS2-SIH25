{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Air Quality Forecasting - Final Training Script\n",
        "\n",
        "# This script trains and evaluates the final champion models based on the findings from the exploratory notebook.\n",
        "# It includes:\n",
        "# 1. A strong XGBoost baseline model.\n",
        "# 2. Two specialized, hyperparameter-tuned Temporal LSTM models (one for O3, one for NO2).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hPRp5JB2gkzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 1: Imports and Global Setup"
      ],
      "metadata": {
        "id": "NyUAKHmMgvW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fztyiZ4FgaWU",
        "outputId": "99fa57f7-81c6-47d1-ab1b-c7d6144de3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Importing core libraries...\n",
            "[DEBUG] Libraries loaded.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"[DEBUG] Importing core libraries...\")\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "print(\"[DEBUG] Libraries loaded.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 2: Global Configuration & Best Hyperparameters"
      ],
      "metadata": {
        "id": "OEppg5V2g5xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Setting global configuration...\")\n",
        "# --- Path and Data Configuration ---\n",
        "BASE_DIR = Path(\"./PS2-SIH25\").resolve()\n",
        "DATA_DIR = BASE_DIR / \"Data_SIH_2025 2\"\n",
        "ARTIFACT_DIR = Path(\"artifacts/final_models\")\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SITE_IDS = [f\"site_{i}\" for i in range(1, 8)]\n",
        "TARGET_COLUMNS = [\"O3_target\", \"NO2_target\"]\n",
        "FORECAST_COLUMNS = [\"O3_forecast\", \"NO2_forecast\", \"T_forecast\", \"q_forecast\", \"u_forecast\", \"v_forecast\", \"w_forecast\"]\n",
        "SATELLITE_COLUMNS = [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]\n",
        "TIME_COLUMNS = [\"year\", \"month\", \"day\", \"hour\"]\n",
        "\n",
        "# --- Model & Training Configuration ---\n",
        "RANDOM_SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RESULTS = {}\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# --- Best Optuna Hyperparameters for LSTM Models ---\n",
        "# NOTE: The 'horizon' is set to 48 for both models to meet the final problem statement.\n",
        "BEST_LSTM_PARAMS = {\n",
        "    \"O3_target\": {\n",
        "        'window': 72,\n",
        "        'horizon': 48,\n",
        "        'batch_size': 128,\n",
        "        'hidden_size': 64,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.35,\n",
        "        'lr': 0.0008753,\n",
        "        'patience': 8\n",
        "    },\n",
        "    \"NO2_target\": {\n",
        "        'window': 48,\n",
        "        'horizon': 48,\n",
        "        'batch_size': 96,\n",
        "        'hidden_size': 96,\n",
        "        'num_layers': 1,\n",
        "        'dropout': 0.2,\n",
        "        'lr': 0.0009557,\n",
        "        'patience': 7\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"[DEBUG] Base dir: {BASE_DIR}\")\n",
        "print(f\"[DEBUG] Data dir: {DATA_DIR}\")\n",
        "print(f\"[DEBUG] Artifacts will be saved to: {ARTIFACT_DIR}\")\n",
        "print(f\"[DEBUG] Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Q6NBEYg7sv",
        "outputId": "276a3a7a-79f5-4805-c313-4448bdf44716"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Setting global configuration...\n",
            "[DEBUG] Base dir: /content/PS2-SIH25\n",
            "[DEBUG] Data dir: /content/PS2-SIH25/Data_SIH_2025 2\n",
            "[DEBUG] Artifacts will be saved to: artifacts/final_models\n",
            "[DEBUG] Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Core Data Loading & Utility Functions"
      ],
      "metadata": {
        "id": "DJ2syYNRg-q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Defining data utility helpers...\")\n",
        "\n",
        "def ensure_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def load_site_dataframe(site_id: str) -> pd.DataFrame:\n",
        "    path = DATA_DIR / f\"{site_id}_train_data.csv\"\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"site_id\"] = site_id\n",
        "    df = ensure_timestamp(df)\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
        "    df[numeric_cols] = df[numeric_cols].interpolate().ffill().bfill()\n",
        "    return df\n",
        "\n",
        "def load_all_train_data() -> pd.DataFrame:\n",
        "    frames = [load_site_dataframe(site_id) for site_id in SITE_IDS]\n",
        "    combined = pd.concat(frames, axis=0, ignore_index=True)\n",
        "    combined = combined.sort_values([\"timestamp\", \"site_id\"]).reset_index(drop=True)\n",
        "    print(f\"[DEBUG] Loaded and combined data for {len(SITE_IDS)} sites. Total rows: {len(combined)}\")\n",
        "    return combined\n",
        "\n",
        "def chronological_split(df: pd.DataFrame, split_ratio: float = 0.8):\n",
        "    unique_ts = np.sort(df[\"timestamp\"].unique())\n",
        "    cutoff_index = int(len(unique_ts) * split_ratio)\n",
        "    cutoff = unique_ts[cutoff_index]\n",
        "    train_mask = df[\"timestamp\"] <= cutoff\n",
        "    train_df = df.loc[train_mask].reset_index(drop=True)\n",
        "    val_df = df.loc[~train_mask].reset_index(drop=True)\n",
        "    print(f\"[DEBUG] Chronological split at {cutoff} -> train {len(train_df)}, val {len(val_df)}\")\n",
        "    return train_df, val_df\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"mae\": float(mae), \"rmse\": float(rmse), \"r2\": float(r2)}\n",
        "\n",
        "print(\"[DEBUG] Utility functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RdZW8MMhBnt",
        "outputId": "24fb1885-3fd1-4159-da94-7cdd2da5e608"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Defining data utility helpers...\n",
            "[DEBUG] Utility functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: Initial Data Loading"
      ],
      "metadata": {
        "id": "jj7WahIvhFQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Loading and preparing the main dataframe...\")\n",
        "ALL_TRAIN_DF = load_all_train_data()\n",
        "print(\"[DEBUG] Main dataframe ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB3ArCjFhHhT",
        "outputId": "6f855602-85ca-4d1e-e262-60dad42f0b95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Loading and preparing the main dataframe...\n",
            "[DEBUG] Loaded and combined data for 7 sites. Total rows: 171679\n",
            "[DEBUG] Main dataframe ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5: XGBoost Model - Feature Engineering"
      ],
      "metadata": {
        "id": "-HFdCDCzhJDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Defining feature engineering for XGBoost model...\")\n",
        "\n",
        "def add_time_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"hour_sin\"] = np.sin(2 * math.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * math.pi * df[\"hour\"] / 24)\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    df[\"dow_sin\"] = np.sin(2 * math.pi * df[\"dayofweek\"] / 7)\n",
        "    df[\"dow_cos\"] = np.cos(2 * math.pi * df[\"dayofweek\"] / 7)\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df: pd.DataFrame, columns: list[str], window: int) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in columns:\n",
        "        for lag in range(1, window + 1):\n",
        "            df[f\"{col}_lag_{lag}\"] = df.groupby(\"site_id\")[col].shift(lag)\n",
        "    return df\n",
        "\n",
        "def build_tree_dataset(base_df: pd.DataFrame, target_col: str, horizon: int, lag_window: int) -> pd.DataFrame:\n",
        "    records = []\n",
        "    feature_columns_base = TIME_COLUMNS + FORECAST_COLUMNS + SATELLITE_COLUMNS\n",
        "    for site_id, site_df in base_df.groupby(\"site_id\"):\n",
        "        enriched = add_time_signals(site_df)\n",
        "        enriched = add_lag_features(enriched, feature_columns_base, lag_window)\n",
        "        enriched[\"site_numeric\"] = int(site_id.split(\"_\")[1])\n",
        "\n",
        "        feature_cols = [col for col in enriched.columns if col not in TARGET_COLUMNS + [\"timestamp\", \"site_id\"]]\n",
        "        base_frame = enriched[[\"timestamp\", \"site_id\"] + feature_cols].copy()\n",
        "\n",
        "        for h in range(1, horizon + 1):\n",
        "            horizon_frame = base_frame.copy()\n",
        "            horizon_frame[\"horizon\"] = h\n",
        "            horizon_frame[\"target\"] = enriched[target_col].shift(-h)\n",
        "            records.append(horizon_frame)\n",
        "\n",
        "    dataset = pd.concat(records, ignore_index=True)\n",
        "    dataset = dataset.dropna(subset=[\"target\"])\n",
        "    print(f\"[DEBUG] Built tree dataset for {target_col}: {dataset.shape}\")\n",
        "    return dataset\n",
        "\n",
        "def prepare_tree_matrices(df: pd.DataFrame):\n",
        "    feature_cols = [col for col in df.columns if col not in [\"timestamp\", \"target\", \"site_id\"]]\n",
        "    X = df[feature_cols].values\n",
        "    y = df[\"target\"].values\n",
        "    return X, y, feature_cols\n",
        "\n",
        "print(\"[DEBUG] XGBoost feature functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXUn4dnyhL5M",
        "outputId": "d1b40499-88cd-4c7e-9591-60e4153d5231"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Defining feature engineering for XGBoost model...\n",
            "[DEBUG] XGBoost feature functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6: XGBoost Model - Training, Evaluation & Saving"
      ],
      "metadata": {
        "id": "FoXO8-kghNz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"### Training XGBoost Baseline Models ###\")\n",
        "print(\"=\"*50 + \"\\\\n\")\n",
        "\n",
        "import pickle # <-- 1. IMPORT THE PICKLE LIBRARY\n",
        "\n",
        "RESULTS[\"xgboost\"] = {}\n",
        "XGB_MODELS = {}\n",
        "\n",
        "for target_col in TARGET_COLUMNS:\n",
        "    print(f\"--- Training XGBoost for {target_col} ---\")\n",
        "    dataset = build_tree_dataset(ALL_TRAIN_DF, target_col, horizon=24, lag_window=24)\n",
        "    train_df, val_df = chronological_split(dataset, split_ratio=0.8)\n",
        "\n",
        "    X_train, y_train, feature_cols = prepare_tree_matrices(train_df)\n",
        "    X_val, y_val, _ = prepare_tree_matrices(val_df)\n",
        "\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=400,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=RANDOM_SEED,\n",
        "        tree_method=\"hist\",\n",
        "        n_jobs=-1,\n",
        "        early_stopping_rounds=30\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    metrics = evaluate_predictions(y_val, preds)\n",
        "    RESULTS[\"xgboost\"][target_col] = metrics\n",
        "\n",
        "    # --- CHANGE: Save a checkpoint dictionary using pickle ---\n",
        "\n",
        "    # 2. Create a dictionary containing the model and its features\n",
        "    checkpoint = {\n",
        "        \"model\": model,\n",
        "        \"feature_cols\": feature_cols\n",
        "    }\n",
        "\n",
        "    # 3. Change the file extension to .pkl\n",
        "    model_save_path = ARTIFACT_DIR / f\"xgboost_{target_col}_checkpoint.pkl\"\n",
        "\n",
        "    # 4. Save the checkpoint dictionary with pickle.dump\n",
        "    with open(model_save_path, 'wb') as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "\n",
        "    print(f\"[INFO] Saved XGBoost checkpoint for {target_col} to {model_save_path}\")\n",
        "    print(f\"[INFO] XGBoost metrics for {target_col} -> MAE={metrics['mae']:.4f}, RMSE={metrics['rmse']:.4f}, R2={metrics['r2']:.4f}\\\\n\")\n",
        "\n",
        "# --- Calculate and store macro averages ---\n",
        "macro_mae = np.mean([m['mae'] for m in RESULTS[\"xgboost\"].values()])\n",
        "macro_rmse = np.mean([m['rmse'] for m in RESULTS[\"xgboost\"].values()])\n",
        "macro_r2 = np.mean([m['r2'] for m in RESULTS[\"xgboost\"].values()])\n",
        "RESULTS[\"xgboost\"][\"macro\"] = {\"mae\": macro_mae, \"rmse\": macro_rmse, \"r2\": macro_r2}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hzN25rdr0ev",
        "outputId": "9b1dd08b-61da-4307-e2da-a4b094b30e2b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n==================================================\n",
            "### Training XGBoost Baseline Models ###\n",
            "==================================================\\n\n",
            "--- Training XGBoost for O3_target ---\n",
            "[DEBUG] Built tree dataset for O3_target: (4118196, 360)\n",
            "[DEBUG] Chronological split at 2023-06-17T14:00:00.000000000 -> train 3289248, val 828948\n",
            "[INFO] Saved XGBoost checkpoint for O3_target to artifacts/final_models/xgboost_O3_target_checkpoint.pkl\n",
            "[INFO] XGBoost metrics for O3_target -> MAE=18.4717, RMSE=28.1014, R2=0.3438\\n\n",
            "--- Training XGBoost for NO2_target ---\n",
            "[DEBUG] Built tree dataset for NO2_target: (4118196, 360)\n",
            "[DEBUG] Chronological split at 2023-06-17T14:00:00.000000000 -> train 3289248, val 828948\n",
            "[INFO] Saved XGBoost checkpoint for NO2_target to artifacts/final_models/xgboost_NO2_target_checkpoint.pkl\n",
            "[INFO] XGBoost metrics for NO2_target -> MAE=17.4989, RMSE=25.6756, R2=0.2207\\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7: Temporal LSTM - Feature, Model & Training Functions"
      ],
      "metadata": {
        "id": "fm1EpduchS5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Defining sequence preparation and LSTM utilities...\")\n",
        "\n",
        "# --- Feature and Sequence Generation ---\n",
        "def add_rolling_features(df: pd.DataFrame, columns: list[str], windows: list[int]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in columns:\n",
        "        if col not in df.columns: continue\n",
        "        for window in windows:\n",
        "            feature_name = f\"{col}_roll_mean_{window}\"\n",
        "            df[feature_name] = df.groupby(\"site_id\")[col].transform(lambda s: s.shift(1).rolling(window, min_periods=1).mean())\n",
        "    return df\n",
        "\n",
        "def build_feature_matrix(df: pd.DataFrame, add_site_one_hot: bool = True) -> tuple[pd.DataFrame, list[str]]:\n",
        "    base = df.copy()\n",
        "    base = add_time_signals(base)\n",
        "    if add_site_one_hot:\n",
        "        site_dummies = pd.get_dummies(base[\"site_id\"], prefix=\"site\")\n",
        "        base = pd.concat([base, site_dummies], axis=1)\n",
        "    feature_cols = [col for col in base.columns if col not in TARGET_COLUMNS + [\"timestamp\", \"site_id\"]]\n",
        "    return base, feature_cols\n",
        "\n",
        "def generate_sequences(df: pd.DataFrame, feature_cols: list[str], window: int, horizon: int) -> list[dict]:\n",
        "    sequences = []\n",
        "    for site_id, site_df in df.groupby(\"site_id\"):\n",
        "        site_df = site_df.reset_index(drop=True)\n",
        "        feat_vals = site_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "        targ_vals = site_df[TARGET_COLUMNS].to_numpy(dtype=np.float32)\n",
        "        for idx in range(window, len(site_df) - horizon):\n",
        "            x_window = feat_vals[idx - window: idx]\n",
        "            y_window = targ_vals[idx: idx + horizon]\n",
        "            if not np.all(np.isfinite(x_window)) or not np.all(np.isfinite(y_window)): continue\n",
        "            sequences.append({\"x\": x_window, \"y\": y_window})\n",
        "    print(f\"[DEBUG] Generated {len(sequences)} sequences (window={window}, horizon={horizon})\")\n",
        "    return sequences\n",
        "\n",
        "def project_sequences_to_target(sequences, target_col):\n",
        "    idx = TARGET_COLUMNS.index(target_col)\n",
        "    return [{**seq, 'y': seq['y'][:, [idx]]} for seq in sequences]\n",
        "\n",
        "def split_sequences_lstm(sequences: list, train_ratio=0.7, val_ratio=0.15):\n",
        "    total = len(sequences)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "    return sequences[:train_end], sequences[train_end:val_end], sequences[val_end:]\n",
        "\n",
        "# --- Scaling ---\n",
        "def fit_scalers(train_sequences: list[dict]):\n",
        "    train_features = np.concatenate([seq[\"x\"] for seq in train_sequences], axis=0)\n",
        "    train_targets = np.concatenate([seq[\"y\"] for seq in train_sequences], axis=0)\n",
        "    feature_scaler = StandardScaler().fit(train_features)\n",
        "    target_scaler = StandardScaler().fit(train_targets)\n",
        "    return feature_scaler, target_scaler\n",
        "\n",
        "def apply_scalers(sequences: list[dict], feature_scaler: StandardScaler, target_scaler: StandardScaler):\n",
        "    return [{**seq, \"x\": feature_scaler.transform(seq[\"x\"]), \"y\": target_scaler.transform(seq[\"y\"])} for seq in sequences]\n",
        "\n",
        "def inverse_transform_predictions(preds: np.ndarray, scaler: StandardScaler) -> np.ndarray:\n",
        "    flat = preds.reshape(-1, preds.shape[-1])\n",
        "    restored = scaler.inverse_transform(flat)\n",
        "    return restored.reshape(preds.shape)\n",
        "\n",
        "# --- PyTorch Components ---\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences: list[dict]):\n",
        "        self.features = torch.tensor(np.stack([seq[\"x\"] for seq in sequences]), dtype=torch.float32)\n",
        "        self.targets = torch.tensor(np.stack([seq[\"y\"] for seq in sequences]), dtype=torch.float32)\n",
        "    def __len__(self): return self.features.shape[0]\n",
        "    def __getitem__(self, idx: int): return self.features[idx], self.targets[idx]\n",
        "\n",
        "class TemporalLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, horizon, target_dim, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.horizon, self.target_dim = horizon, target_dim\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, horizon * target_dim)\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        preds = self.fc(output[:, -1, :])\n",
        "        return preds.view(-1, self.horizon, self.target_dim)\n",
        "\n",
        "def train_lstm(model, train_loader, val_loader, epochs=40, lr=1e-3, patience=5, weight_decay=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # Added weight_decay\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_state = None\n",
        "    stale_epochs = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                val_losses.append(criterion(model(xb), yb).item())\n",
        "\n",
        "        val_loss = np.mean(val_losses)\n",
        "        print(f\"[DEBUG] Epoch {epoch:02d} | val_loss={val_loss:.5f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            stale_epochs = 0\n",
        "        else:\n",
        "            stale_epochs += 1\n",
        "            if stale_epochs >= patience:\n",
        "                print(f\"[DEBUG] Early stopping triggered after {epoch} epochs.\")\n",
        "                break\n",
        "\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "print(\"[DEBUG] LSTM utilities defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lowW1CxPg3mw",
        "outputId": "a408682a-be5d-448a-884a-dee24292bfaf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Defining sequence preparation and LSTM utilities...\n",
            "[DEBUG] LSTM utilities defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8: Temporal LSTM - Final Training, Evaluation & Saving"
      ],
      "metadata": {
        "id": "LqXzN-lahY-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"### Training Tuned Temporal LSTM Models ###\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "RESULTS[\"tuned_lstm\"] = {}\n",
        "FINAL_LSTM_MODELS = {}\n",
        "\n",
        "# --- Prepare the base dataframe with all possible features once ---\n",
        "lstm_base_df = add_rolling_features(ALL_TRAIN_DF, FORECAST_COLUMNS + SATELLITE_COLUMNS + TARGET_COLUMNS, [6, 24])\n",
        "base_feature_df, all_lstm_feature_cols = build_feature_matrix(lstm_base_df)\n",
        "\n",
        "for target_col in TARGET_COLUMNS:\n",
        "    print(f\"--- Training Tuned LSTM for {target_col} ---\")\n",
        "    params = BEST_LSTM_PARAMS[target_col]\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    sequence_records = generate_sequences(base_feature_df, all_lstm_feature_cols, params['window'], params['horizon'])\n",
        "    projected_sequences = project_sequences_to_target(sequence_records, target_col)\n",
        "\n",
        "    train_seq, val_seq, test_seq = split_sequences_lstm(projected_sequences)\n",
        "\n",
        "    feature_scaler, target_scaler = fit_scalers(train_seq)\n",
        "\n",
        "    train_scaled = apply_scalers(train_seq, feature_scaler, target_scaler)\n",
        "    val_scaled = apply_scalers(val_seq, feature_scaler, target_scaler)\n",
        "    test_scaled = apply_scalers(test_seq, feature_scaler, target_scaler)\n",
        "\n",
        "    train_loader = DataLoader(SequenceDataset(train_scaled), batch_size=params['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(SequenceDataset(val_scaled), batch_size=params['batch_size'])\n",
        "    test_loader = DataLoader(SequenceDataset(test_scaled), batch_size=params['batch_size'])\n",
        "\n",
        "    # --- Model Training ---\n",
        "    model = TemporalLSTM(\n",
        "        input_size=len(all_lstm_feature_cols),\n",
        "        hidden_size=params['hidden_size'],\n",
        "        horizon=params['horizon'],\n",
        "        target_dim=1,\n",
        "        num_layers=params['num_layers'],\n",
        "        dropout=params['dropout']\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    model = train_lstm(model, train_loader, val_loader, epochs=40, lr=params['lr'], patience=params['patience'])\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            all_preds.append(model(xb.to(DEVICE)).cpu().numpy())\n",
        "            all_targets.append(yb.numpy())\n",
        "\n",
        "    pred_array = np.concatenate(all_preds)\n",
        "    target_array = np.concatenate(all_targets)\n",
        "\n",
        "    pred_restored = inverse_transform_predictions(pred_array, target_scaler)\n",
        "    target_restored = inverse_transform_predictions(target_array, target_scaler)\n",
        "\n",
        "    metrics = evaluate_predictions(target_restored.flatten(), pred_restored.flatten())\n",
        "    RESULTS[\"tuned_lstm\"][target_col] = metrics\n",
        "\n",
        "    # --- Save the Champion LSTM Model ---\n",
        "    checkpoint = {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"params\": params,\n",
        "        \"feature_cols\": all_lstm_feature_cols,  # <-- FIX: Added the missing feature list here\n",
        "        \"target_scaler\": target_scaler,\n",
        "        \"feature_scaler\": feature_scaler,\n",
        "        \"metrics\": metrics,\n",
        "        \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "    model_save_path = ARTIFACT_DIR / f\"lstm_{target_col}_champion.pt\"\n",
        "    torch.save(checkpoint, model_save_path)\n",
        "\n",
        "    print(f\"[INFO] Saved Tuned LSTM model for {target_col} to {model_save_path}\")\n",
        "    print(f\"[INFO] Tuned LSTM metrics for {target_col} -> MAE={metrics['mae']:.4f}, RMSE={metrics['rmse']:.4f}, R2={metrics['r2']:.4f}\\n\")\n",
        "\n",
        "# --- Calculate and store macro averages ---\n",
        "macro_mae_lstm = np.mean([m['mae'] for m in RESULTS[\"tuned_lstm\"].values()])\n",
        "macro_rmse_lstm = np.mean([m['rmse'] for m in RESULTS[\"tuned_lstm\"].values()])\n",
        "macro_r2_lstm = np.mean([m['r2'] for m in RESULTS[\"tuned_lstm\"].values()])\n",
        "RESULTS[\"tuned_lstm\"][\"macro\"] = {\"mae\": macro_mae_lstm, \"rmse\": macro_rmse_lstm, \"r2\": macro_r2_lstm}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVvTPMXahb0e",
        "outputId": "5f7fdc54-5341-4364-ff58-e0b4a7c1a85c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "### Training Tuned Temporal LSTM Models ###\n",
            "==================================================\n",
            "\n",
            "--- Training Tuned LSTM for O3_target ---\n",
            "[DEBUG] Generated 170832 sequences (window=72, horizon=48)\n",
            "[DEBUG] Epoch 01 | val_loss=0.19943\n",
            "[DEBUG] Epoch 02 | val_loss=0.21271\n",
            "[DEBUG] Epoch 03 | val_loss=0.22928\n",
            "[DEBUG] Epoch 04 | val_loss=0.22140\n",
            "[DEBUG] Epoch 05 | val_loss=0.21308\n",
            "[DEBUG] Epoch 06 | val_loss=0.22505\n",
            "[DEBUG] Epoch 07 | val_loss=0.21688\n",
            "[DEBUG] Epoch 08 | val_loss=0.22331\n",
            "[DEBUG] Epoch 09 | val_loss=0.22237\n",
            "[DEBUG] Early stopping triggered after 9 epochs.\n",
            "[INFO] Saved Tuned LSTM model for O3_target to artifacts/final_models/lstm_O3_target_champion.pt\n",
            "[INFO] Tuned LSTM metrics for O3_target -> MAE=14.7698, RMSE=21.7439, R2=0.6659\n",
            "\n",
            "--- Training Tuned LSTM for NO2_target ---\n",
            "[DEBUG] Generated 171000 sequences (window=48, horizon=48)\n",
            "[DEBUG] Epoch 01 | val_loss=0.27028\n",
            "[DEBUG] Epoch 02 | val_loss=0.31302\n",
            "[DEBUG] Epoch 03 | val_loss=0.31550\n",
            "[DEBUG] Epoch 04 | val_loss=0.31165\n",
            "[DEBUG] Epoch 05 | val_loss=0.31100\n",
            "[DEBUG] Epoch 06 | val_loss=0.33749\n",
            "[DEBUG] Epoch 07 | val_loss=0.35033\n",
            "[DEBUG] Epoch 08 | val_loss=0.33709\n",
            "[DEBUG] Early stopping triggered after 8 epochs.\n",
            "[INFO] Saved Tuned LSTM model for NO2_target to artifacts/final_models/lstm_NO2_target_champion.pt\n",
            "[INFO] Tuned LSTM metrics for NO2_target -> MAE=13.1157, RMSE=18.5710, R2=0.3849\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9: Final Results Summary"
      ],
      "metadata": {
        "id": "EWEAlX2Khc7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"### Final Model Comparison ###\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "summary_records = []\n",
        "for model_name, model_results in RESULTS.items():\n",
        "    for target_name, metrics in model_results.items():\n",
        "        summary_records.append({\n",
        "            \"model\": model_name,\n",
        "            \"target\": target_name,\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "print(summary_df.to_string())\n",
        "\n",
        "# --- Declare the final champion based on macro MAE ---\n",
        "xgb_mae = RESULTS.get(\"xgboost\", {}).get(\"macro\", {}).get(\"mae\", float('inf'))\n",
        "lstm_mae = RESULTS.get(\"tuned_lstm\", {}).get(\"macro\", {}).get(\"mae\", float('inf'))\n",
        "\n",
        "print(\"\\n---\")\n",
        "if lstm_mae < xgb_mae:\n",
        "    print(f\"[CHAMPION] The Tuned LSTM model is the winner with a macro MAE of {lstm_mae:.4f} (vs. XGBoost's {xgb_mae:.4f})\")\n",
        "else:\n",
        "    print(f\"[CHAMPION] The XGBoost model is the winner with a macro MAE of {xgb_mae:.4f} (vs. Tuned LSTM's {lstm_mae:.4f})\")\n",
        "print(\"---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhbqJ76_hXtt",
        "outputId": "4c7179ca-54ac-4c32-9c22-7768a5dd4800"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "### Final Model Comparison ###\n",
            "==================================================\n",
            "\n",
            "        model      target        mae       rmse        r2\n",
            "0     xgboost   O3_target  18.471678  28.101396  0.343793\n",
            "1     xgboost  NO2_target  17.498871  25.675592  0.220721\n",
            "2     xgboost       macro  17.985274  26.888494  0.282257\n",
            "3  tuned_lstm   O3_target  14.806224  21.640255  0.669037\n",
            "4  tuned_lstm  NO2_target  12.410555  17.837553  0.432495\n",
            "5  tuned_lstm       macro  13.608389  19.738904  0.550766\n",
            "\n",
            "---\n",
            "[CHAMPION] The Tuned LSTM model is the winner with a macro MAE of 13.6084 (vs. XGBoost's 17.9853)\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Imports and Global Setup"
      ],
      "metadata": {
        "id": "-yIIPmrjo9Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"[DEBUG] Libraries loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgn2Knu-o-8a",
        "outputId": "540a3166-55e7-4f12-cf0d-331597cc9ae1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Libraries loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Configuration - Point to Models and Data"
      ],
      "metadata": {
        "id": "NskFWFPNpBeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Setting configuration...\")\n",
        "\n",
        "# --- Configure paths ---\n",
        "# Ensure these paths match where your models and data are stored\n",
        "BASE_DIR = Path(\"./PS2-SIH25\").resolve()\n",
        "DATA_DIR = BASE_DIR / \"Data_SIH_2025 2\"\n",
        "ARTIFACT_DIR = Path(\"artifacts/final_models\")\n",
        "\n",
        "# --- Select a site to test ---\n",
        "SITE_ID_TO_TEST = \"site_1\"\n",
        "\n",
        "# --- Define column groups (should match your training script) ---\n",
        "TARGET_COLUMNS = [\"O3_target\", \"NO2_target\"]\n",
        "FORECAST_COLUMNS = [\"O3_forecast\", \"NO2_forecast\", \"T_forecast\", \"q_forecast\", \"u_forecast\", \"v_forecast\", \"w_forecast\"]\n",
        "SATELLITE_COLUMNS = [\"NO2_satellite\", \"HCHO_satellite\", \"ratio_satellite\"]\n",
        "TIME_COLUMNS = [\"year\", \"month\", \"day\", \"hour\"]\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"[INFO] Running test case for site: {SITE_ID_TO_TEST}\")\n",
        "print(f\"[INFO] Using device: {DEVICE}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyqCsj0ho4lt",
        "outputId": "4c31450e-aafe-44ab-b16e-439ecb3bff52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Setting configuration...\n",
            "[INFO] Running test case for site: site_1\n",
            "[INFO] Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Utility Functions & Model Class Definition\n"
      ],
      "metadata": {
        "id": "NOHkIXropIjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[DEBUG] Defining utility functions and LSTM class...\")\n",
        "\n",
        "def ensure_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def load_dataframe(site_id: str, suffix: str) -> pd.DataFrame:\n",
        "    path = DATA_DIR / f\"{site_id}{suffix}\"\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"site_id\"] = site_id\n",
        "    df = ensure_timestamp(df)\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
        "    df[numeric_cols] = df[numeric_cols].interpolate().ffill().bfill()\n",
        "    return df\n",
        "\n",
        "def add_time_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"hour_sin\"] = np.sin(2 * math.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * math.pi * df[\"hour\"] / 24)\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
        "    df[\"dow_sin\"] = np.sin(2 * math.pi * df[\"dayofweek\"] / 7)\n",
        "    df[\"dow_cos\"] = np.cos(2 * math.pi * df[\"dayofweek\"] / 7)\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df: pd.DataFrame, columns: list[str], window: int) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in columns:\n",
        "        for lag in range(1, window + 1):\n",
        "            df[f\"{col}_lag_{lag}\"] = df.groupby(\"site_id\")[col].shift(lag)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df: pd.DataFrame, columns: list[str], windows: list[int]) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in columns:\n",
        "        if col not in df.columns: continue\n",
        "        for window in windows:\n",
        "            feature_name = f\"{col}_roll_mean_{window}\"\n",
        "            df[feature_name] = df.groupby(\"site_id\")[col].transform(lambda s: s.shift(1).rolling(window, min_periods=1).mean())\n",
        "    return df\n",
        "\n",
        "# Define the LSTM model class EXACTLY as it was during training\n",
        "class TemporalLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, horizon, target_dim, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.horizon, self.target_dim = horizon, target_dim\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, horizon * target_dim)\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        preds = self.fc(output[:, -1, :])\n",
        "        return preds.view(-1, self.horizon, self.target_dim)\n",
        "\n",
        "print(\"[DEBUG] Utilities ready.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xvicYI3pG5m",
        "outputId": "df16a27f-9071-4b95-ce56-438db887c564"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Defining utility functions and LSTM class...\n",
            "[DEBUG] Utilities ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: Load Historical and Unseen Data"
      ],
      "metadata": {
        "id": "AxR_R4I_pN6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tail of the training data and all of the unseen data\n",
        "train_df = load_dataframe(SITE_ID_TO_TEST, \"_train_data.csv\")\n",
        "unseen_df = load_dataframe(SITE_ID_TO_TEST, \"_unseen_input_data.csv\")\n",
        "\n",
        "# We need enough history for the longest lookback window (72 for O3 LSTM)\n",
        "# We'll take the last 200 hours of training data as a safe buffer\n",
        "historical_context_df = train_df.tail(200)\n",
        "\n",
        "# The input data for prediction will be this historical context followed by the unseen data\n",
        "input_df = pd.concat([historical_context_df, unseen_df], ignore_index=True)\n",
        "\n",
        "print(f\"[INFO] Prepared input data of shape: {input_df.shape}\")\n",
        "print(f\"[INFO] Forecasting from the last known timestamp: {unseen_df['timestamp'].max()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipo5B9n_pPQA",
        "outputId": "c95d4dfd-58b2-42d4-bb23-f9ce044158aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Prepared input data of shape: (11072, 18)\n",
            "[INFO] Forecasting from the last known timestamp: 2024-06-29 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5: Test Case for XGBoost Models"
      ],
      "metadata": {
        "id": "GBs6xC-SpXOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"### Testing XGBoost Models (with Pickle) ###\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "try:\n",
        "    # --- Prepare data as before ---\n",
        "    xgb_input_df = input_df.copy()\n",
        "    rolling_feature_cols = FORECAST_COLUMNS + SATELLITE_COLUMNS + TARGET_COLUMNS\n",
        "    xgb_input_df = add_rolling_features(xgb_input_df, rolling_feature_cols, [6, 24])\n",
        "    xgb_input_df = add_time_signals(xgb_input_df)\n",
        "\n",
        "    # --- FIX: Added TIME_COLUMNS to the list of features to be lagged ---\n",
        "    lag_feature_cols = TIME_COLUMNS + FORECAST_COLUMNS + SATELLITE_COLUMNS\n",
        "\n",
        "    xgb_input_df = add_lag_features(xgb_input_df, lag_feature_cols, 24)\n",
        "    xgb_input_df[\"site_numeric\"] = int(SITE_ID_TO_TEST.split(\"_\")[1])\n",
        "\n",
        "    prediction_row = xgb_input_df.dropna().tail(1)\n",
        "\n",
        "    if prediction_row.empty:\n",
        "        print(\"\\n[FATAL ERROR] The dataframe is empty after dropping NaNs.\")\n",
        "    else:\n",
        "        for target_col in TARGET_COLUMNS:\n",
        "            print(f\"\\n--- Loading and predicting for {target_col} with XGBoost ---\")\n",
        "\n",
        "            # --- Load Checkpoint from Pickle File ---\n",
        "            checkpoint_path = ARTIFACT_DIR / f\"xgboost_{target_col}_checkpoint.pkl\"\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                checkpoint = pickle.load(f)\n",
        "\n",
        "            model = checkpoint[\"model\"]\n",
        "            feature_names_from_training = checkpoint[\"feature_cols\"]\n",
        "            print(f\"[INFO] Loaded checkpoint for {target_col} from {checkpoint_path}\")\n",
        "\n",
        "            # --- Make Prediction ---\n",
        "            prediction_df = pd.concat([prediction_row] * 24, ignore_index=True)\n",
        "            prediction_df['horizon'] = np.arange(1, 25)\n",
        "\n",
        "            X_test = prediction_df[feature_names_from_training]\n",
        "\n",
        "            predictions = model.predict(X_test)\n",
        "\n",
        "            print(f\"[XGBoost] 24-Hour Forecast for {target_col}:\")\n",
        "            for i in range(24):\n",
        "                print(f\"  - Hour T+{i+1}: {predictions[i]:.4f}\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mND382fIpVoI",
        "outputId": "19934f85-b88f-4f8a-c5d8-5549ace26c21"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "### Testing XGBoost Models (with Pickle) ###\n",
            "==================================================\n",
            "\n",
            "\n",
            "--- Loading and predicting for O3_target with XGBoost ---\n",
            "[INFO] Loaded checkpoint for O3_target from artifacts/final_models/xgboost_O3_target_checkpoint.pkl\n",
            "[XGBoost] 24-Hour Forecast for O3_target:\n",
            "  - Hour T+1: -4.4780\n",
            "  - Hour T+2: -4.4373\n",
            "  - Hour T+3: -2.3704\n",
            "  - Hour T+4: 7.6461\n",
            "  - Hour T+5: 19.7956\n",
            "  - Hour T+6: 34.0199\n",
            "  - Hour T+7: 37.3157\n",
            "  - Hour T+8: 41.6491\n",
            "  - Hour T+9: 41.6381\n",
            "  - Hour T+10: 41.9554\n",
            "  - Hour T+11: 37.6496\n",
            "  - Hour T+12: 29.8153\n",
            "  - Hour T+13: 21.7244\n",
            "  - Hour T+14: 17.8686\n",
            "  - Hour T+15: 10.3316\n",
            "  - Hour T+16: 3.7187\n",
            "  - Hour T+17: 2.5342\n",
            "  - Hour T+18: 0.1007\n",
            "  - Hour T+19: -2.1686\n",
            "  - Hour T+20: -6.0530\n",
            "  - Hour T+21: -8.4266\n",
            "  - Hour T+22: -9.5110\n",
            "  - Hour T+23: -10.4798\n",
            "  - Hour T+24: -10.1351\n",
            "--------------------\n",
            "\n",
            "--- Loading and predicting for NO2_target with XGBoost ---\n",
            "[INFO] Loaded checkpoint for NO2_target from artifacts/final_models/xgboost_NO2_target_checkpoint.pkl\n",
            "[XGBoost] 24-Hour Forecast for NO2_target:\n",
            "  - Hour T+1: 19.8153\n",
            "  - Hour T+2: 19.8153\n",
            "  - Hour T+3: 19.8153\n",
            "  - Hour T+4: 19.8153\n",
            "  - Hour T+5: 19.8153\n",
            "  - Hour T+6: 19.8153\n",
            "  - Hour T+7: 19.8153\n",
            "  - Hour T+8: 19.8153\n",
            "  - Hour T+9: 19.8153\n",
            "  - Hour T+10: 19.8153\n",
            "  - Hour T+11: 19.8153\n",
            "  - Hour T+12: 19.8153\n",
            "  - Hour T+13: 19.8153\n",
            "  - Hour T+14: 19.8153\n",
            "  - Hour T+15: 19.8153\n",
            "  - Hour T+16: 19.8153\n",
            "  - Hour T+17: 19.8153\n",
            "  - Hour T+18: 19.8153\n",
            "  - Hour T+19: 19.8153\n",
            "  - Hour T+20: 19.8153\n",
            "  - Hour T+21: 19.8153\n",
            "  - Hour T+22: 19.8153\n",
            "  - Hour T+23: 19.8153\n",
            "  - Hour T+24: 19.8153\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6: Test Case for Tuned LSTM Models"
      ],
      "metadata": {
        "id": "-28kacIrpbf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"### Testing Tuned LSTM Models ###\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "for target_col in TARGET_COLUMNS:\n",
        "    print(f\"--- Loading and predicting for {target_col} with LSTM ---\")\n",
        "\n",
        "    # --- Load Checkpoint ---\n",
        "    checkpoint_path = ARTIFACT_DIR / f\"lstm_{target_col}_champion.pt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
        "\n",
        "    # --- Recreate Model from Checkpoint ---\n",
        "    params = checkpoint['params']\n",
        "    model = TemporalLSTM(\n",
        "        input_size=len(checkpoint['feature_cols']),\n",
        "        hidden_size=params['hidden_size'],\n",
        "        horizon=params['horizon'],\n",
        "        target_dim=1,\n",
        "        num_layers=params['num_layers'],\n",
        "        dropout=params['dropout']\n",
        "    ).to(DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    model.eval()\n",
        "\n",
        "    # --- Load Scalers ---\n",
        "    feature_scaler = checkpoint['feature_scaler']\n",
        "    target_scaler = checkpoint['target_scaler']\n",
        "\n",
        "    # --- Prepare data specifically for this LSTM ---\n",
        "    lstm_input_df = add_rolling_features(input_df, FORECAST_COLUMNS + SATELLITE_COLUMNS + TARGET_COLUMNS, [6, 24])\n",
        "    lstm_feature_df = add_time_signals(lstm_input_df)\n",
        "\n",
        "    site_dummies = pd.get_dummies(lstm_feature_df[\"site_id\"], prefix=\"site\")\n",
        "    lstm_feature_df = pd.concat([lstm_feature_df, site_dummies], axis=1)\n",
        "\n",
        "    inference_window = lstm_feature_df.tail(params['window'])\n",
        "\n",
        "    X_test_raw = inference_window.reindex(columns=checkpoint['feature_cols'], fill_value=0)\n",
        "\n",
        "    X_test_scaled = feature_scaler.transform(X_test_raw)\n",
        "\n",
        "    # --- FIX: Add a check for NaNs and fill them ---\n",
        "    if np.isnan(X_test_scaled).any():\n",
        "        print(\"[WARNING] NaNs found in the scaled input data! This is likely the cause of 'nan' predictions.\")\n",
        "        print(\"[INFO] Attempting to fix by filling NaNs with 0 (mean value for StandardScaler)...\")\n",
        "        X_test_scaled = np.nan_to_num(X_test_scaled)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # --- Make Prediction ---\n",
        "    with torch.no_grad():\n",
        "        prediction_scaled = model(X_test_tensor)\n",
        "\n",
        "    # Inverse transform the prediction to get the real value\n",
        "    prediction_restored = target_scaler.inverse_transform(prediction_scaled.cpu().numpy().reshape(-1, 1))\n",
        "\n",
        "    print(f\"[LSTM] 48-Hour Forecast for {target_col}:\")\n",
        "    for i in range(params['horizon']):\n",
        "        print(f\"  - Hour T+{i+1}: {prediction_restored[i][0]:.4f}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg4_E3DVpb4S",
        "outputId": "f06f8258-bc14-4464-d6ad-2df030d630b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "### Testing Tuned LSTM Models ###\n",
            "==================================================\n",
            "\n",
            "--- Loading and predicting for O3_target with LSTM ---\n",
            "[WARNING] NaNs found in the scaled input data! This is likely the cause of 'nan' predictions.\n",
            "[INFO] Attempting to fix by filling NaNs with 0 (mean value for StandardScaler)...\n",
            "[LSTM] 48-Hour Forecast for O3_target:\n",
            "  - Hour T+1: 13.8075\n",
            "  - Hour T+2: 14.8228\n",
            "  - Hour T+3: 14.7176\n",
            "  - Hour T+4: 16.3125\n",
            "  - Hour T+5: 16.4259\n",
            "  - Hour T+6: 22.3516\n",
            "  - Hour T+7: 26.9032\n",
            "  - Hour T+8: 32.3826\n",
            "  - Hour T+9: 37.8943\n",
            "  - Hour T+10: 39.6267\n",
            "  - Hour T+11: 39.1313\n",
            "  - Hour T+12: 38.5669\n",
            "  - Hour T+13: 32.4675\n",
            "  - Hour T+14: 28.4669\n",
            "  - Hour T+15: 23.0522\n",
            "  - Hour T+16: 20.6748\n",
            "  - Hour T+17: 18.4595\n",
            "  - Hour T+18: 17.6999\n",
            "  - Hour T+19: 16.6041\n",
            "  - Hour T+20: 14.8703\n",
            "  - Hour T+21: 12.9130\n",
            "  - Hour T+22: 15.3206\n",
            "  - Hour T+23: 12.5122\n",
            "  - Hour T+24: 13.1581\n",
            "  - Hour T+25: 15.2290\n",
            "  - Hour T+26: 13.3412\n",
            "  - Hour T+27: 14.8522\n",
            "  - Hour T+28: 16.5999\n",
            "  - Hour T+29: 19.6744\n",
            "  - Hour T+30: 21.2216\n",
            "  - Hour T+31: 27.1930\n",
            "  - Hour T+32: 34.7249\n",
            "  - Hour T+33: 39.4960\n",
            "  - Hour T+34: 41.4625\n",
            "  - Hour T+35: 40.3628\n",
            "  - Hour T+36: 36.2885\n",
            "  - Hour T+37: 34.3015\n",
            "  - Hour T+38: 27.9014\n",
            "  - Hour T+39: 23.9801\n",
            "  - Hour T+40: 20.5223\n",
            "  - Hour T+41: 18.4090\n",
            "  - Hour T+42: 16.4664\n",
            "  - Hour T+43: 16.0134\n",
            "  - Hour T+44: 17.7995\n",
            "  - Hour T+45: 16.0419\n",
            "  - Hour T+46: 12.9207\n",
            "  - Hour T+47: 12.9558\n",
            "  - Hour T+48: 13.3721\n",
            "--------------------\n",
            "--- Loading and predicting for NO2_target with LSTM ---\n",
            "[WARNING] NaNs found in the scaled input data! This is likely the cause of 'nan' predictions.\n",
            "[INFO] Attempting to fix by filling NaNs with 0 (mean value for StandardScaler)...\n",
            "[LSTM] 48-Hour Forecast for NO2_target:\n",
            "  - Hour T+1: 37.9815\n",
            "  - Hour T+2: 38.5158\n",
            "  - Hour T+3: 37.8049\n",
            "  - Hour T+4: 33.0274\n",
            "  - Hour T+5: 30.6005\n",
            "  - Hour T+6: 28.4362\n",
            "  - Hour T+7: 24.8778\n",
            "  - Hour T+8: 19.9664\n",
            "  - Hour T+9: 13.1322\n",
            "  - Hour T+10: 10.1889\n",
            "  - Hour T+11: 7.4225\n",
            "  - Hour T+12: 12.7813\n",
            "  - Hour T+13: 16.9871\n",
            "  - Hour T+14: 28.1765\n",
            "  - Hour T+15: 43.5691\n",
            "  - Hour T+16: 53.4428\n",
            "  - Hour T+17: 62.0898\n",
            "  - Hour T+18: 65.8155\n",
            "  - Hour T+19: 62.7646\n",
            "  - Hour T+20: 58.0669\n",
            "  - Hour T+21: 54.4343\n",
            "  - Hour T+22: 47.3642\n",
            "  - Hour T+23: 42.5215\n",
            "  - Hour T+24: 42.3801\n",
            "  - Hour T+25: 40.1508\n",
            "  - Hour T+26: 41.0126\n",
            "  - Hour T+27: 37.8542\n",
            "  - Hour T+28: 36.7415\n",
            "  - Hour T+29: 33.9044\n",
            "  - Hour T+30: 29.4332\n",
            "  - Hour T+31: 24.4253\n",
            "  - Hour T+32: 19.4449\n",
            "  - Hour T+33: 14.3470\n",
            "  - Hour T+34: 12.2222\n",
            "  - Hour T+35: 11.3008\n",
            "  - Hour T+36: 13.9906\n",
            "  - Hour T+37: 21.7201\n",
            "  - Hour T+38: 33.0204\n",
            "  - Hour T+39: 48.0423\n",
            "  - Hour T+40: 57.0184\n",
            "  - Hour T+41: 63.6009\n",
            "  - Hour T+42: 67.5318\n",
            "  - Hour T+43: 63.1955\n",
            "  - Hour T+44: 60.1342\n",
            "  - Hour T+45: 54.8205\n",
            "  - Hour T+46: 48.3989\n",
            "  - Hour T+47: 46.5393\n",
            "  - Hour T+48: 43.8119\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3dfH9WSxWXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}